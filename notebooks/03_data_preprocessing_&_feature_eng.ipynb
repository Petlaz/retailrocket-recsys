{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f0bcd8",
   "metadata": {},
   "source": [
    "## Step 3. Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b4113e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw dataframe shape: (2756101, 5)\n",
      "       timestamp  visitorid event  itemid  transactionid\n",
      "0  1433221332117     257597  view  355908            NaN\n",
      "1  1433224214164     992329  view  248676            NaN\n",
      "2  1433221999827     111016  view  318965            NaN\n",
      "3  1433221955914     483717  view  253185            NaN\n",
      "4  1433221337106     951259  view  367447            NaN\n",
      "Step 1: Data duplicated for preprocessing. Number of rows: 2756101\n",
      "Duplicate (itemid, property) pairs: 8786767\n",
      "Sample duplicate rows:\n",
      "        timestamp  itemid   property                            value\n",
      "1   1441508400000  206783        888          1116713 960601 n277.200\n",
      "2   1439089200000  395014        400  n552.000 639502 n720.000 424566\n",
      "3   1431226800000   59481        790                       n15360.000\n",
      "5   1436065200000  285026  available                                0\n",
      "9   1434250800000   98113        451                  1141052 n48.000\n",
      "10  1439089200000  450113        888           1038400 45956 n504.000\n",
      "11  1435460400000  244127        400  n552.000 639502 n720.000 424566\n",
      "14  1434250800000  169055        790                       n21000.000\n",
      "15  1437274800000  186518  available                                0\n",
      "17  1436670000000  319291        888                          1292080\n",
      "\n",
      "Missing values after filling:\n",
      "property\n",
      "itemid              0\n",
      "0              406388\n",
      "1              411404\n",
      "10             416585\n",
      "100            416847\n",
      "                ...  \n",
      "999            416836\n",
      "available           0\n",
      "categoryid          0\n",
      "category            0\n",
      "description         0\n",
      "Length: 1107, dtype: int64\n",
      "Cleaned item properties saved. Shape: (417053, 1107)\n",
      "\n",
      "Extracting datetime features from 'timestamp'...\n",
      "       timestamp                datetime\n",
      "0  1433221332117 2015-06-02 05:02:12.117\n",
      "1  1433224214164 2015-06-02 05:50:14.164\n",
      "2  1433221999827 2015-06-02 05:13:19.827\n",
      "3  1433221955914 2015-06-02 05:12:35.914\n",
      "4  1433221337106 2015-06-02 05:02:17.106\n",
      "Rows with invalid datetime: 0\n",
      "Rows after dropping invalid datetime: 2756101\n",
      "Datetime features extracted:\n",
      "   visitorid                datetime  day  hour  weekday  month\n",
      "0     257597 2015-06-02 05:02:12.117    2     5        1      6\n",
      "1     992329 2015-06-02 05:50:14.164    2     5        1      6\n",
      "2     111016 2015-06-02 05:13:19.827    2     5        1      6\n",
      "3     483717 2015-06-02 05:12:35.914    2     5        1      6\n",
      "4     951259 2015-06-02 05:02:17.106    2     5        1      6\n",
      "Last event per visitor sample:\n",
      "   visitorid         last_event_time\n",
      "0          0 2015-09-11 20:55:17.175\n",
      "1          1 2015-08-13 17:46:06.444\n",
      "2          2 2015-08-07 18:20:57.845\n",
      "3          3 2015-08-01 07:10:35.296\n",
      "4          4 2015-09-15 21:24:27.167\n",
      "Sample with recency_days:\n",
      "   visitorid                datetime         last_event_time  recency_days\n",
      "0     257597 2015-06-02 05:02:12.117 2015-06-08 22:00:21.247             6\n",
      "1     992329 2015-06-02 05:50:14.164 2015-07-30 16:23:01.735            58\n",
      "2     111016 2015-06-02 05:13:19.827 2015-06-02 05:13:19.827             0\n",
      "3     483717 2015-06-02 05:12:35.914 2015-06-02 05:12:35.914             0\n",
      "4     951259 2015-06-02 05:02:17.106 2015-06-02 05:02:17.106             0\n",
      "Step 3 preprocessing completed and saved.\n",
      "\n",
      "Encoding categorical item properties...\n",
      "Loaded item properties rows: 10999999\n",
      "Duplicate (itemid, property) pairs: 4407664\n",
      "Rows after aggregation: 6592335\n",
      "Pivoted to wide format.\n",
      "Categorical columns for encoding: ['categoryid', 'available']\n",
      "Encoded item properties shape: (417053, 2272)\n",
      "Encoded item properties saved.\n",
      "\n",
      "Removing users and items with fewer than 5 interactions...\n",
      "Number of users before filtering: 1407580\n",
      "Number of items before filtering: 103873\n",
      "Data shape after filtering outliers: (833463, 12)\n",
      "Filtered data saved to ../data/processed/df_filtered.pkl\n",
      "Step 3 Data Preprocessing & Feature Engineering completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load raw gzipped CSV (this avoids datetime conversion issues)\n",
    "file_path = \"../data/raw/retailrocket_events.csv.gz\"\n",
    "df = pd.read_csv(file_path, compression='gzip', low_memory=False)\n",
    "print(f\"Loaded raw dataframe shape: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "### 1. Duplicate data for preprocessing safety\n",
    "df_preproc = df.copy()\n",
    "print(f\"Step 1: Data duplicated for preprocessing. Number of rows: {len(df_preproc)}\")\n",
    "\n",
    "### 2. Handle missing values in item metadata (categories, descriptions)\n",
    "item_props_1 = pd.read_csv(\"../data/raw/item_properties_part1.csv\")\n",
    "item_props_2 = pd.read_csv(\"../data/raw/item_properties_part2.csv\")\n",
    "\n",
    "item_props = pd.concat([item_props_1, item_props_2])\n",
    "duplicates = item_props.duplicated(subset=['itemid', 'property'], keep=False)\n",
    "print(f\"Duplicate (itemid, property) pairs: {duplicates.sum()}\")\n",
    "print(\"Sample duplicate rows:\")\n",
    "print(item_props[duplicates].head(10))\n",
    "\n",
    "# Aggregate duplicates by first occurrence\n",
    "item_props_agg = item_props.groupby(['itemid', 'property'])['value'].first().reset_index()\n",
    "\n",
    "# Pivot to wide format\n",
    "item_props_wide = item_props_agg.pivot(index='itemid', columns='property', values='value').reset_index()\n",
    "\n",
    "# Fill missing values\n",
    "if 'category' in item_props_wide.columns:\n",
    "    item_props_wide['category'] = item_props_wide['category'].fillna('unknown')\n",
    "else:\n",
    "    item_props_wide['category'] = 'unknown'\n",
    "\n",
    "if 'description' in item_props_wide.columns:\n",
    "    item_props_wide['description'] = item_props_wide['description'].fillna('')\n",
    "else:\n",
    "    item_props_wide['description'] = ''\n",
    "\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(item_props_wide.isna().sum())\n",
    "\n",
    "item_props_wide.to_parquet(\"../data/processed/item_properties_wide.parquet\")\n",
    "print(f\"Cleaned item properties saved. Shape: {item_props_wide.shape}\")\n",
    "\n",
    "### 3. Extract timestamp features from df_preproc\n",
    "print(\"\\nExtracting datetime features from 'timestamp'...\")\n",
    "df_preproc['datetime'] = pd.to_datetime(df_preproc['timestamp'], unit='ms', errors='coerce')\n",
    "print(df_preproc[['timestamp', 'datetime']].head())\n",
    "\n",
    "invalid_datetime_count = df_preproc['datetime'].isna().sum()\n",
    "print(f\"Rows with invalid datetime: {invalid_datetime_count}\")\n",
    "df_preproc = df_preproc.dropna(subset=['datetime'])\n",
    "print(f\"Rows after dropping invalid datetime: {len(df_preproc)}\")\n",
    "\n",
    "# Extract datetime components\n",
    "df_preproc['day'] = df_preproc['datetime'].dt.day\n",
    "df_preproc['hour'] = df_preproc['datetime'].dt.hour\n",
    "df_preproc['weekday'] = df_preproc['datetime'].dt.weekday\n",
    "df_preproc['month'] = df_preproc['datetime'].dt.month\n",
    "\n",
    "print(\"Datetime features extracted:\")\n",
    "print(df_preproc[['visitorid', 'datetime', 'day', 'hour', 'weekday', 'month']].head())\n",
    "\n",
    "# Calculate recency (days since last event per user)\n",
    "last_event = df_preproc.groupby('visitorid')['datetime'].max().reset_index()\n",
    "last_event.rename(columns={'datetime': 'last_event_time'}, inplace=True)\n",
    "print(\"Last event per visitor sample:\")\n",
    "print(last_event.head())\n",
    "\n",
    "df_preproc = df_preproc.merge(last_event, on='visitorid', how='left')\n",
    "df_preproc['recency_days'] = (df_preproc['last_event_time'] - df_preproc['datetime']).dt.days\n",
    "print(\"Sample with recency_days:\")\n",
    "print(df_preproc[['visitorid', 'datetime', 'last_event_time', 'recency_days']].head())\n",
    "\n",
    "# Save preprocessed dataframe for further steps\n",
    "df_preproc.to_parquet(\"../data/processed/df_preprocessed.parquet\")\n",
    "print(\"Step 3 preprocessing completed and saved.\")\n",
    "\n",
    "### 4. Item metadata categorical encoding\n",
    "print(\"\\nEncoding categorical item properties...\")\n",
    "\n",
    "item_props = pd.read_csv(\"../data/raw/item_properties_part1.csv\", usecols=['itemid', 'property', 'value'])\n",
    "print(f\"Loaded item properties rows: {len(item_props)}\")\n",
    "\n",
    "dupes_count = item_props.duplicated(subset=['itemid', 'property']).sum()\n",
    "print(f\"Duplicate (itemid, property) pairs: {dupes_count}\")\n",
    "\n",
    "# Aggregate duplicates by mode of 'value'\n",
    "item_props_agg = (\n",
    "    item_props.groupby(['itemid', 'property'])['value']\n",
    "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Rows after aggregation: {len(item_props_agg)}\")\n",
    "\n",
    "# Pivot to wide format again for encoding\n",
    "item_props_wide = item_props_agg.pivot(index='itemid', columns='property', values='value').reset_index()\n",
    "print(\"Pivoted to wide format.\")\n",
    "\n",
    "# Select categorical columns (example: 'categoryid' and 'available')\n",
    "categorical_cols = [col for col in ['categoryid', 'available'] if col in item_props_wide.columns]\n",
    "print(f\"Categorical columns for encoding: {categorical_cols}\")\n",
    "\n",
    "# One-hot encode if any categorical columns found\n",
    "if categorical_cols:\n",
    "    item_props_wide[categorical_cols] = item_props_wide[categorical_cols].fillna('missing').astype(str)\n",
    "    item_props_encoded = pd.get_dummies(item_props_wide, columns=categorical_cols, dummy_na=False)\n",
    "else:\n",
    "    item_props_encoded = item_props_wide.copy()\n",
    "    print(\"No categorical columns found for encoding.\")\n",
    "\n",
    "print(f\"Encoded item properties shape: {item_props_encoded.shape}\")\n",
    "item_props_encoded.to_pickle(\"../data/processed/item_properties_encoded.pkl\")\n",
    "print(\"Encoded item properties saved.\")\n",
    "\n",
    "### 5. Remove outliers (users/items with fewer than 5 interactions)\n",
    "print(\"\\nRemoving users and items with fewer than 5 interactions...\")\n",
    "\n",
    "user_interactions = df_preproc['visitorid'].value_counts()\n",
    "print(f\"Number of users before filtering: {len(user_interactions)}\")\n",
    "valid_users = user_interactions[user_interactions >= 5].index\n",
    "df_filtered = df_preproc[df_preproc['visitorid'].isin(valid_users)]\n",
    "\n",
    "item_interactions = df_filtered['itemid'].value_counts()\n",
    "print(f\"Number of items before filtering: {len(item_interactions)}\")\n",
    "valid_items = item_interactions[item_interactions >= 5].index\n",
    "df_filtered = df_filtered[df_filtered['itemid'].isin(valid_items)]\n",
    "\n",
    "print(f\"Data shape after filtering outliers: {df_filtered.shape}\")\n",
    "\n",
    "# Save filtered dataset for modeling\n",
    "cleaned_path = '../data/processed/df_filtered.pkl'\n",
    "df_filtered.to_pickle(cleaned_path)\n",
    "print(f\"Filtered data saved to {cleaned_path}\")\n",
    "\n",
    "print(\"Step 3 Data Preprocessing & Feature Engineering completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0343b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
