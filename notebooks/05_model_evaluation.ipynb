{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39b2b85",
   "metadata": {},
   "source": [
    "## 5 — Model Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a75aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered interaction data...\n",
      "Loaded filtered data with shape: (833463, 12)\n",
      "Loading popularity model...\n",
      "Loading content-based similarity subset...\n",
      "Loaded 8967 items in content-based similarity subset.\n",
      "Loading KNN collaborative filtering model...\n",
      "KNN model loaded.\n",
      "Train shape: (681915, 12), Test shape: (151548, 12)\n",
      "Building sparse user-item matrix from TRAIN data...\n",
      "Sparse TRAIN matrix shape: users=80112, items=38975, non-zeros=393640\n",
      "\n",
      "Evaluating Popularity-based recommender on up to 10000 users...\n",
      "Evaluating Content-Based recommender on up to 10000 users...\n",
      "Evaluating KNN Collaborative Filtering recommender on up to 10000 users...\n",
      "\n",
      "Evaluation Results (Top-K = 10):\n",
      "                         Model  Precision@K  Recall@K       MAP      NDCG  \\\n",
      "0                   Popularity     0.001488  0.008161  0.003680  0.005270   \n",
      "1                Content-Based     0.000175  0.000755  0.000282  0.000494   \n",
      "2  KNN Collaborative Filtering     0.021086  0.143621  0.062179  0.086425   \n",
      "\n",
      "   UsersEvaluated  \n",
      "0            9812  \n",
      "1            7987  \n",
      "2            9812  \n",
      "\n",
      "Best performing model based on MAP: KNN Collaborative Filtering\n",
      "Model             KNN Collaborative Filtering\n",
      "Precision@K                          0.021086\n",
      "Recall@K                             0.143621\n",
      "MAP                                  0.062179\n",
      "NDCG                                 0.086425\n",
      "UsersEvaluated                           9812\n",
      "Name: 2, dtype: object\n",
      "Saved evaluation results to ../artifacts/models/model_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# === Paths (relative to notebooks/) ===\n",
    "DATA_FILTERED_PATH = \"../data/processed/df_filtered.pkl\"\n",
    "ARTIFACTS_DIR = \"../artifacts\"\n",
    "MODELS_DIR = os.path.join(ARTIFACTS_DIR, \"models\")\n",
    "EMBEDDINGS_DIR = os.path.join(ARTIFACTS_DIR, \"embeddings\")\n",
    "\n",
    "POPULARITY_PATH = os.path.join(MODELS_DIR, \"popularity_model.parquet\")\n",
    "KNN_MODEL_PATH = os.path.join(MODELS_DIR, \"knn_model.pkl\")\n",
    "CB_SIM_PATH = os.path.join(EMBEDDINGS_DIR, \"item_similarity_sub.npy\")\n",
    "CB_ITEMS_PATH = os.path.join(EMBEDDINGS_DIR, \"item_similarity_sub_items.csv\")\n",
    "\n",
    "# === Load Data and Models ===\n",
    "print(\"Loading filtered interaction data...\")\n",
    "df_filtered = pd.read_pickle(DATA_FILTERED_PATH)\n",
    "print(f\"Loaded filtered data with shape: {df_filtered.shape}\")\n",
    "\n",
    "print(\"Loading popularity model...\")\n",
    "popularity_df = pd.read_parquet(POPULARITY_PATH).sort_values(\"count\", ascending=False)\n",
    "popular_items = popularity_df[\"itemid\"].tolist()\n",
    "\n",
    "print(\"Loading content-based similarity subset...\")\n",
    "item_sim_matrix = np.load(CB_SIM_PATH, mmap_mode=\"r\")\n",
    "subset_items = pd.read_csv(CB_ITEMS_PATH, header=None).iloc[:, 0].tolist()\n",
    "subset_item_to_idx = {item: idx for idx, item in enumerate(subset_items)}\n",
    "print(f\"Loaded {len(subset_items)} items in content-based similarity subset.\")\n",
    "\n",
    "print(\"Loading KNN collaborative filtering model...\")\n",
    "with open(KNN_MODEL_PATH, \"rb\") as f:\n",
    "    knn_model: NearestNeighbors = pickle.load(f)\n",
    "print(\"KNN model loaded.\")\n",
    "\n",
    "# === Train/Test Split (user-wise) ===\n",
    "def train_test_split_userwise(df, test_ratio=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_parts, test_parts = [], []\n",
    "    for user_id, group in df.groupby(\"visitorid\"):\n",
    "        if len(group) < 2:\n",
    "            train_parts.append(group)\n",
    "            continue\n",
    "        test_size = max(1, int(len(group) * test_ratio))\n",
    "        test_indices = rng.choice(group.index.values, size=test_size, replace=False)\n",
    "        test_parts.append(group.loc[test_indices])\n",
    "        train_parts.append(group.drop(test_indices))\n",
    "    return pd.concat(train_parts), pd.concat(test_parts)\n",
    "\n",
    "train_df, test_df = train_test_split_userwise(df_filtered, test_ratio=0.2, seed=42)\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "# === Prepare Ground Truth for Evaluation ===\n",
    "test_user_items = defaultdict(set)\n",
    "for row in test_df.itertuples(index=False):\n",
    "    test_user_items[row.visitorid].add(row.itemid)\n",
    "\n",
    "# === Build Sparse User-Item Interaction Matrix for TRAIN ===\n",
    "def build_sparse_user_item(df):\n",
    "    users_cat = df[\"visitorid\"].astype(\"category\")\n",
    "    items_cat = df[\"itemid\"].astype(\"category\")\n",
    "    u_codes = users_cat.cat.codes\n",
    "    i_codes = items_cat.cat.codes\n",
    "    counts = (\n",
    "        pd.DataFrame({\"u\": u_codes, \"i\": i_codes})\n",
    "        .value_counts()\n",
    "        .reset_index(name=\"c\")\n",
    "    )\n",
    "    n_users = users_cat.cat.categories.size\n",
    "    n_items = items_cat.cat.categories.size\n",
    "\n",
    "    mat = coo_matrix(\n",
    "        (counts[\"c\"].values, (counts[\"u\"].values, counts[\"i\"].values)),\n",
    "        shape=(n_users, n_items),\n",
    "    ).tocsr()\n",
    "\n",
    "    user_idx_to_id = users_cat.cat.categories\n",
    "    item_idx_to_id = items_cat.cat.categories\n",
    "    user_id_to_idx = {uid: i for i, uid in enumerate(user_idx_to_id)}\n",
    "    item_id_to_idx = {iid: i for i, iid in enumerate(item_idx_to_id)}\n",
    "    return mat, user_idx_to_id, item_idx_to_id, user_id_to_idx, item_id_to_idx\n",
    "\n",
    "print(\"Building sparse user-item matrix from TRAIN data...\")\n",
    "user_item_csr, u_idx2id, i_idx2id, u_id2idx, i_id2idx = build_sparse_user_item(train_df)\n",
    "print(f\"Sparse TRAIN matrix shape: users={user_item_csr.shape[0]}, items={user_item_csr.shape[1]}, non-zeros={user_item_csr.nnz}\")\n",
    "\n",
    "# === Check KNN model compatibility ===\n",
    "expected_features = getattr(knn_model, \"n_features_in_\", None)\n",
    "if expected_features is not None and expected_features != user_item_csr.shape[0]:\n",
    "    print(f\"WARNING: KNN model expects {expected_features} features, but user-item matrix has {user_item_csr.shape[0]} users.\")\n",
    "    print(\"Skipping KNN evaluation due to incompatibility.\")\n",
    "    knn_model = None\n",
    "\n",
    "# === Recommendation Functions ===\n",
    "def recommend_popularity(_seed_item, top_n=10):\n",
    "    # Simply return top popular items regardless of seed item\n",
    "    return popular_items[:top_n]\n",
    "\n",
    "def recommend_content_based(seed_item, top_n=10):\n",
    "    idx = subset_item_to_idx.get(seed_item)\n",
    "    if idx is None:\n",
    "        return []\n",
    "    sims = item_sim_matrix[idx]\n",
    "    # Get indices of top similar items (excluding seed itself)\n",
    "    top_indices = np.argpartition(-sims, range(top_n + 1))[:top_n + 1]\n",
    "    top_indices = [i for i in top_indices if i != idx][:top_n]\n",
    "    top_indices = sorted(top_indices, key=lambda i: sims[i], reverse=True)[:top_n]\n",
    "    return [subset_items[i] for i in top_indices]\n",
    "\n",
    "def recommend_knn(seed_item, top_n=10):\n",
    "    if knn_model is None:\n",
    "        return []\n",
    "    item_idx = i_id2idx.get(seed_item)\n",
    "    if item_idx is None:\n",
    "        return []\n",
    "    # Extract the item vector (column) as dense row vector shape (1, num_users)\n",
    "    vec = user_item_csr[:, item_idx].toarray().reshape(1, -1)\n",
    "    # Verify dimensions match knn_model input feature count\n",
    "    expected_dim = getattr(knn_model, \"n_features_in_\", None)\n",
    "    if expected_dim is not None and expected_dim != vec.shape[1]:\n",
    "        print(f\"WARNING: KNN query vector dimension {vec.shape[1]} does not match model expected {expected_dim}.\")\n",
    "        return []\n",
    "    distances, indices = knn_model.kneighbors(vec, n_neighbors=top_n + 1)\n",
    "    rec_idxs = [j for j in indices.flatten() if j != item_idx][:top_n]\n",
    "    # Defensive: clip indices to valid range\n",
    "    max_idx = len(i_idx2id) - 1\n",
    "    rec_idxs = [min(max(0, idx), max_idx) for idx in rec_idxs]\n",
    "    return [i_idx2id[idx] for idx in rec_idxs]\n",
    "\n",
    "# === Evaluation Metrics ===\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "    return len(set(recommended[:k]) & relevant) / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len(set(recommended[:k]) & relevant) / len(relevant)\n",
    "\n",
    "def average_precision(recommended, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    ap = 0.0\n",
    "    hits = 0\n",
    "    for rank, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            ap += hits / rank\n",
    "    return ap / min(len(relevant), k)\n",
    "\n",
    "def ndcg_at_k(recommended, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(recommended[:k], start=1):\n",
    "        if item in relevant:\n",
    "            dcg += 1.0 / math.log2(rank + 1)\n",
    "    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, min(len(relevant), k) + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "def evaluate(recommender_fn, users, k=10, seed=42, max_users=None, require_seed_in_domain=False):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if max_users is not None and len(users) > max_users:\n",
    "        users = rng.choice(users, size=max_users, replace=False)\n",
    "\n",
    "    precisions, recalls, maps, ndcgs = [], [], [], []\n",
    "    train_user_items = train_df.groupby(\"visitorid\")[\"itemid\"].apply(set).to_dict()\n",
    "\n",
    "    for user in users:\n",
    "        if user not in test_user_items:\n",
    "            continue\n",
    "        relevant = test_user_items[user]\n",
    "        if not relevant:\n",
    "            continue\n",
    "        seed_pool = train_user_items.get(user, set())\n",
    "        if not seed_pool:\n",
    "            continue\n",
    "\n",
    "        # Pick a seed item from user's training history\n",
    "        if require_seed_in_domain:\n",
    "            # Restrict seed to items in content-based similarity subset\n",
    "            candidates = list(seed_pool & set(subset_items))\n",
    "            if not candidates:\n",
    "                continue\n",
    "            seed_item = rng.choice(candidates)\n",
    "        else:\n",
    "            seed_item = rng.choice(list(seed_pool))\n",
    "\n",
    "        recommended = recommender_fn(seed_item, top_n=k)\n",
    "        if not recommended:\n",
    "            continue\n",
    "\n",
    "        precisions.append(precision_at_k(recommended, relevant, k))\n",
    "        recalls.append(recall_at_k(recommended, relevant, k))\n",
    "        maps.append(average_precision(recommended, relevant, k))\n",
    "        ndcgs.append(ndcg_at_k(recommended, relevant, k))\n",
    "\n",
    "    return {\n",
    "        \"Precision@K\": float(np.mean(precisions)) if precisions else 0.0,\n",
    "        \"Recall@K\": float(np.mean(recalls)) if recalls else 0.0,\n",
    "        \"MAP\": float(np.mean(maps)) if maps else 0.0,\n",
    "        \"NDCG\": float(np.mean(ndcgs)) if ndcgs else 0.0,\n",
    "        \"UsersEvaluated\": len(precisions),\n",
    "    }\n",
    "\n",
    "# === Run Evaluation ===\n",
    "all_train_users = train_df[\"visitorid\"].unique()\n",
    "K = 10\n",
    "MAX_EVAL_USERS = 10000\n",
    "\n",
    "print(f\"\\nEvaluating Popularity-based recommender on up to {MAX_EVAL_USERS} users...\")\n",
    "pop_metrics = evaluate(recommend_popularity, all_train_users, k=K, max_users=MAX_EVAL_USERS)\n",
    "\n",
    "print(f\"Evaluating Content-Based recommender on up to {MAX_EVAL_USERS} users...\")\n",
    "cb_metrics = evaluate(recommend_content_based, all_train_users, k=K, max_users=MAX_EVAL_USERS, require_seed_in_domain=True)\n",
    "\n",
    "print(f\"Evaluating KNN Collaborative Filtering recommender on up to {MAX_EVAL_USERS} users...\")\n",
    "knn_metrics = {}\n",
    "if knn_model is not None:\n",
    "    knn_metrics = evaluate(recommend_knn, all_train_users, k=K, max_users=MAX_EVAL_USERS)\n",
    "else:\n",
    "    print(\"KNN model unavailable or incompatible, skipping evaluation.\")\n",
    "\n",
    "# === Summarize Results ===\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Model\": \"Popularity\", **pop_metrics},\n",
    "    {\"Model\": \"Content-Based\", **cb_metrics},\n",
    "])\n",
    "if knn_metrics:\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\"Model\": \"KNN Collaborative Filtering\", **knn_metrics}])], ignore_index=True)\n",
    "\n",
    "print(\"\\nEvaluation Results (Top-K = {}):\".format(K))\n",
    "print(results_df)\n",
    "\n",
    "# === Select Best Model ===\n",
    "best_model_row = results_df.loc[results_df[\"MAP\"].idxmax()]\n",
    "print(f\"\\nBest performing model based on MAP: {best_model_row['Model']}\")\n",
    "print(best_model_row)\n",
    "\n",
    "# === Save evaluation results to CSV ===\n",
    "eval_results_path = os.path.join(MODELS_DIR, \"model_evaluation_results.csv\")\n",
    "results_df.to_csv(eval_results_path, index=False)\n",
    "print(f\"Saved evaluation results to {eval_results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a8efc",
   "metadata": {},
   "source": [
    "## Step 6 — Hyperparameter Tuning for KNN Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1a4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting KNN hyperparameter tuning with 12 combinations...\n",
      "Training KNN with n_neighbors=5, algorithm='auto', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=5, algorithm='auto', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Training KNN with n_neighbors=5, algorithm='brute', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=5, algorithm='brute', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Training KNN with n_neighbors=10, algorithm='auto', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=10, algorithm='auto', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Training KNN with n_neighbors=10, algorithm='brute', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=10, algorithm='brute', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Training KNN with n_neighbors=20, algorithm='auto', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=20, algorithm='auto', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Training KNN with n_neighbors=20, algorithm='brute', metric='cosine'...\n",
      "MAP: 0.0733\n",
      "Training KNN with n_neighbors=20, algorithm='brute', metric='euclidean'...\n",
      "MAP: 0.0242\n",
      "Best MAP: 0.0733 with params: {'n_neighbors': 5, 'algorithm': 'auto', 'metric': 'cosine'}\n",
      "Saved tuned KNN model to ../artifacts/models/knn_model_tuned.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Paths \n",
    "ARTIFACTS_DIR = \"../artifacts\"\n",
    "MODELS_DIR = os.path.join(ARTIFACTS_DIR, \"models\")\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "TUNED_KNN_MODEL_PATH = os.path.join(MODELS_DIR, \"knn_model_tuned.pkl\")\n",
    "\n",
    "# Prepare user-item dictionaries for evaluation\n",
    "from collections import defaultdict\n",
    "\n",
    "train_user_items = train_df.groupby(\"visitorid\")[\"itemid\"].apply(set).to_dict()\n",
    "\n",
    "test_user_items = defaultdict(set)\n",
    "for r in test_df.itertuples(index=False):\n",
    "    test_user_items[r.visitorid].add(r.itemid)\n",
    "\n",
    "# Existing evaluate function should already be defined here from Step 5\n",
    "\n",
    "def tune_knn_manual(\n",
    "    user_item_csr,\n",
    "    train_user_items,\n",
    "    test_user_items,\n",
    "    k=10,\n",
    "    max_users=5000,\n",
    "    n_neighbors_options=[5, 10, 20],\n",
    "    algorithm_options=[\"auto\", \"brute\"],\n",
    "    metric_options=[\"cosine\", \"euclidean\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Manual hyperparameter tuning for KNN collaborative filtering recommender.\n",
    "    Tries combinations of n_neighbors, algorithm, and metric.\n",
    "    Returns the best trained KNN model, best MAP score, and best params.\n",
    "    \"\"\"\n",
    "\n",
    "    best_map = -1\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"Starting KNN hyperparameter tuning with {len(n_neighbors_options) * len(algorithm_options) * len(metric_options)} combinations...\")\n",
    "\n",
    "    for n_neighbors in n_neighbors_options:\n",
    "        for algorithm in algorithm_options:\n",
    "            for metric in metric_options:\n",
    "                print(f\"Training KNN with n_neighbors={n_neighbors}, algorithm='{algorithm}', metric='{metric}'...\")\n",
    "                try:\n",
    "                    knn = NearestNeighbors(\n",
    "                        n_neighbors=n_neighbors,\n",
    "                        algorithm=algorithm,\n",
    "                        metric=metric,\n",
    "                        n_jobs=-1,\n",
    "                    )\n",
    "                    # Fit on user-item CSR matrix transpose because we're looking for similar items\n",
    "                    knn.fit(user_item_csr.T)\n",
    "\n",
    "                    # Define a recommend function wrapper for evaluation\n",
    "                    def knn_recommender(seed_item, top_n=k):\n",
    "                        item_idx = i_id2idx.get(seed_item, None)\n",
    "                        if item_idx is None:\n",
    "                            return []\n",
    "                        vec = user_item_csr[:, item_idx].T\n",
    "                        distances, indices = knn.kneighbors(vec, n_neighbors=top_n + 1)\n",
    "                        rec_idxs = [j for j in indices.flatten() if j != item_idx][:top_n]\n",
    "                        return list(i_idx2id[rec] for rec in rec_idxs)\n",
    "\n",
    "                    # Evaluate knn model\n",
    "                    metrics = evaluate(knn_recommender, list(train_user_items.keys()), k=k, max_users=max_users)\n",
    "\n",
    "                    current_map = metrics[\"MAP\"]\n",
    "                    print(f\"MAP: {current_map:.4f}\")\n",
    "\n",
    "                    if current_map > best_map:\n",
    "                        best_map = current_map\n",
    "                        best_params = {\n",
    "                            \"n_neighbors\": n_neighbors,\n",
    "                            \"algorithm\": algorithm,\n",
    "                            \"metric\": metric,\n",
    "                        }\n",
    "                        best_model = knn\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error training with params n_neighbors={n_neighbors}, algorithm={algorithm}, metric={metric}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    print(f\"Best MAP: {best_map:.4f} with params: {best_params}\")\n",
    "\n",
    "    return best_model, best_map, best_params\n",
    "\n",
    "\n",
    "# Run tuning\n",
    "best_knn_model, best_map, best_params = tune_knn_manual(\n",
    "    user_item_csr, train_user_items, test_user_items, k=10, max_users=5000\n",
    ")\n",
    "\n",
    "# Save tuned model to disk\n",
    "with open(TUNED_KNN_MODEL_PATH, \"wb\") as f:\n",
    "    pickle.dump(best_knn_model, f)\n",
    "print(f\"Saved tuned KNN model to {TUNED_KNN_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6effc",
   "metadata": {},
   "source": [
    "### loading and testing of the tuned KNN model \n",
    "\n",
    "Verify it performs well on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22db4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tuned KNN model...\n",
      "Tuned KNN model loaded.\n",
      "\n",
      "Evaluating Tuned KNN Collaborative Filtering on up to 10000 users...\n",
      "\n",
      "Tuned KNN Evaluation Results:\n",
      "Precision@K: 0.021749\n",
      "Recall@K: 0.145209\n",
      "MAP: 0.071979\n",
      "NDCG: 0.094960\n",
      "UsersEvaluated: 9812.000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths \n",
    "ARTIFACTS_DIR = \"../artifacts\"\n",
    "MODELS_DIR = os.path.join(ARTIFACTS_DIR, \"models\")\n",
    "TUNED_KNN_MODEL_PATH = os.path.join(MODELS_DIR, \"knn_model_tuned.pkl\")\n",
    "\n",
    "# Load tuned KNN model\n",
    "print(\"Loading tuned KNN model...\")\n",
    "with open(TUNED_KNN_MODEL_PATH, \"rb\") as f:\n",
    "    tuned_knn_model = pickle.load(f)\n",
    "print(\"Tuned KNN model loaded.\")\n",
    "\n",
    "# Define the recommend_knn function using the tuned model\n",
    "def recommend_tuned_knn(seed_item, top_n=10):\n",
    "    item_idx = i_id2idx.get(seed_item, None)\n",
    "    if item_idx is None:\n",
    "        return []\n",
    "    vec = user_item_csr[:, item_idx].T\n",
    "    distances, indices = tuned_knn_model.kneighbors(vec, n_neighbors=top_n + 1)\n",
    "    rec_idxs = [j for j in indices.flatten() if j != item_idx][:top_n]\n",
    "    return list(i_idx2id[rec] for rec in rec_idxs)\n",
    "\n",
    "# Run evaluation on a larger set \n",
    "all_train_users = train_df[\"visitorid\"].unique()\n",
    "K = 10\n",
    "MAX_EVAL_USERS = 10000\n",
    "\n",
    "print(f\"\\nEvaluating Tuned KNN Collaborative Filtering on up to {MAX_EVAL_USERS} users...\")\n",
    "tuned_knn_metrics = evaluate(recommend_tuned_knn, all_train_users, k=K, max_users=MAX_EVAL_USERS)\n",
    "\n",
    "print(\"\\nTuned KNN Evaluation Results:\")\n",
    "for metric, value in tuned_knn_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba01a23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
